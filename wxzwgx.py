import asyncio
import aiohttp
from aiohttp import ClientSession, ClientTimeout, TCPConnector
from bs4 import BeautifulSoup
import time
import logging
from typing import Optional, List, Tuple, Dict, Set
from urllib.parse import urljoin, urlparse, parse_qs
import re
from datetime import datetime
from pathlib import Path
import random
import socket
import ssl


class FastNovelCrawler:
    """é«˜é€Ÿå°è¯´çˆ¬è™«ç±»ï¼Œæ”¯æŒå¼‚æ­¥å¹¶å‘çˆ¬å– - åˆ†é¡µé¡ºåºä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, base_url: str = 'https://www.twinfoo.com',
                 concurrent_limit: int = 2, base_delay: float = 1.5):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            base_url: åŸºç¡€URL
            concurrent_limit: å¹¶å‘è¿æ¥æ•°é™åˆ¶ï¼ˆé™ä½åˆ°2ï¼‰
            base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆå¢åŠ åˆ°1.5ç§’ï¼‰
        """
        self.base_url = base_url
        self.concurrent_limit = concurrent_limit
        self.base_delay = base_delay
        self.chapter_count = 0
        self.total_words = 0
        self.failed_urls: Set[str] = set()
        self.retry_count = 5  # å¢åŠ é‡è¯•æ¬¡æ•°

        # å“åº”æ—¶é—´ç›‘æ§
        self.response_times = []
        self.server_load_factor = 1.0

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # æ›´æ–°è¯·æ±‚å¤´ï¼Œæ·»åŠ æ›´å¤šå­—æ®µ
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }

    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('fast_novel_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def check_domain_availability(self) -> bool:
        """æ£€æŸ¥åŸŸåå¯ç”¨æ€§"""
        try:
            parsed_url = urlparse(self.base_url)
            host = parsed_url.hostname
            socket.gethostbyname(host)
            self.logger.info(f"åŸŸåè§£ææˆåŠŸ: {host}")
            return True
        except socket.gaierror as e:
            self.logger.error(f"åŸŸåè§£æå¤±è´¥: {e}")
            self.logger.info("å»ºè®®æ£€æŸ¥:")
            self.logger.info("1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            self.logger.info("2. DNSè®¾ç½®æ˜¯å¦æ­£ç¡®")
            self.logger.info("3. åŸŸåæ˜¯å¦ä»ç„¶æœ‰æ•ˆ")
            return False

    async def diagnose_network_issue(self) -> None:
        """è¯Šæ–­ç½‘ç»œé—®é¢˜"""
        self.logger.info("å¼€å§‹ç½‘ç»œè¯Šæ–­...")

        # è§£æURL
        parsed_url = urlparse(self.base_url)
        host = parsed_url.hostname
        port = parsed_url.port or (443 if parsed_url.scheme == 'https' else 80)

        # 1. DNSè§£ææµ‹è¯•
        try:
            ip = socket.gethostbyname(host)
            self.logger.info(f"âœ… DNSè§£ææˆåŠŸ: {host} -> {ip}")
        except socket.gaierror as e:
            self.logger.error(f"âŒ DNSè§£æå¤±è´¥: {e}")
            return

        # 2. ç«¯å£è¿é€šæ€§æµ‹è¯•
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(10)
            result = sock.connect_ex((host, port))
            sock.close()

            if result == 0:
                self.logger.info(f"âœ… ç«¯å£ {port} è¿é€š")
            else:
                self.logger.error(f"âŒ ç«¯å£ {port} ä¸é€š")
                return
        except Exception as e:
            self.logger.error(f"âŒ ç«¯å£æµ‹è¯•å¤±è´¥: {e}")
            return

        # 3. HTTPè¯·æ±‚æµ‹è¯•
        try:
            async with await self.create_session() as session:
                async with session.get(self.base_url, timeout=30) as response:
                    self.logger.info(f"âœ… HTTPè¯·æ±‚æˆåŠŸ: {response.status}")
        except Exception as e:
            self.logger.error(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")

    def _calculate_adaptive_delay(self) -> float:
        """è®¡ç®—è‡ªé€‚åº”å»¶è¿Ÿæ—¶é—´ - ä¿®å¤ç‰ˆæœ¬"""
        if not self.response_times:
            return self.base_delay * 2  # å¢åŠ åŸºç¡€å»¶è¿Ÿ

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        avg_response_time = sum(self.response_times[-10:]) / len(self.response_times[-10:])

        # æ ¹æ®å“åº”æ—¶é—´è°ƒæ•´å»¶è¿Ÿ - æ›´ä¿å®ˆçš„ç­–ç•¥
        if avg_response_time > 5.0:  # æœåŠ¡å™¨å¾ˆæ…¢
            self.server_load_factor = 3.0
        elif avg_response_time > 3.0:  # æœåŠ¡å™¨è¾ƒæ…¢
            self.server_load_factor = 2.5
        elif avg_response_time > 1.5:  # æœåŠ¡å™¨ä¸­ç­‰è´Ÿè½½
            self.server_load_factor = 2.0
        else:  # æœåŠ¡å™¨å“åº”å¿«
            self.server_load_factor = 1.5  # å³ä½¿å¿«ä¹Ÿä¿æŒè°¨æ…

        # æ·»åŠ éšæœºæ€§ï¼Œé¿å…è§„å¾‹æ€§è¯·æ±‚
        base_delay = self.base_delay * self.server_load_factor
        random_factor = random.uniform(1.0, 2.0)  # å¢åŠ éšæœºèŒƒå›´

        return base_delay * random_factor

    async def create_session(self) -> ClientSession:
        """åˆ›å»ºå¼‚æ­¥HTTPä¼šè¯ - ä¿®å¤ç‰ˆæœ¬"""
        # åˆ›å»ºæ›´å®½æ¾çš„SSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        # å¢åŠ è¶…æ—¶æ—¶é—´
        timeout = ClientTimeout(total=60, connect=20, sock_read=30)

        connector = TCPConnector(
            limit=self.concurrent_limit * 2,
            limit_per_host=self.concurrent_limit,
            keepalive_timeout=60,
            enable_cleanup_closed=True,
            ssl=ssl_context,  # ä½¿ç”¨è‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
            family=socket.AF_INET,  # å¼ºåˆ¶ä½¿ç”¨IPv4
            resolver=None,
            use_dns_cache=True,
            ttl_dns_cache=300,
        )

        return ClientSession(
            timeout=timeout,
            connector=connector,
            headers=self.headers,
            trust_env=True,  # ä¿¡ä»»ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®
        )

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())

        # ç§»é™¤å¸¸è§çš„å¹¿å‘Šæ–‡å­—
        ad_patterns = [
            r'æœ¬ç«™.*?æé†’æ‚¨.*?',
            r'è¯·æ”¶è—.*?',
            r'æ‰‹æœºç”¨æˆ·.*?',
            r'è®°ä½.*?ç½‘å€.*?',
            r'æœ€æ–°ç« èŠ‚.*?',
            r'æ— å¼¹çª—.*?',
            r'.*?é¦–å‘.*?',
            r'.*?æ›´æ–°æœ€å¿«.*?',
        ]

        for pattern in ad_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        return text.strip()

    async def fetch_with_retry(self, session: ClientSession, url: str) -> Optional[str]:
        """å¸¦é‡è¯•çš„HTTPè¯·æ±‚ - ä¿®å¤ç‰ˆæœ¬"""
        for attempt in range(self.retry_count):
            try:
                start_time = time.time()

                # æ·»åŠ æ›´å¤šçš„é”™è¯¯å¤„ç†
                async with session.get(url, allow_redirects=True) as response:
                    if response.status == 200:
                        content = await response.text()
                        response_time = time.time() - start_time
                        self.response_times.append(response_time)

                        if len(self.response_times) > 50:
                            self.response_times = self.response_times[-50:]

                        return content
                    elif response.status == 403:
                        self.logger.warning(f"è®¿é—®è¢«æ‹’ç» 403: {url}")
                        await asyncio.sleep(5)  # ç­‰å¾…æ›´é•¿æ—¶é—´
                    elif response.status == 404:
                        self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨ 404: {url}")
                        break  # 404ä¸éœ€è¦é‡è¯•
                    else:
                        self.logger.warning(f"HTTP {response.status}: {url}")

            except aiohttp.ClientConnectorError as e:
                self.logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.retry_count}): {e}")
                # è¿æ¥é”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(5 * (attempt + 1))
            except aiohttp.ClientSSLError as e:
                self.logger.warning(f"SSLé”™è¯¯ (å°è¯• {attempt + 1}/{self.retry_count}): {e}")
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(3 * (attempt + 1))
            except asyncio.TimeoutError:
                self.logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.retry_count}): {url}")
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(2 * (attempt + 1))
            except Exception as e:
                self.logger.warning(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.retry_count}): {e}")
                if attempt < self.retry_count - 1:
                    await asyncio.sleep(2 ** attempt)

        self.failed_urls.add(url)
        return None

    async def extract_page_content(self, session: ClientSession, url: str) -> Tuple[
        List[str], Optional[str], Optional[str]]:
        """
        å¼‚æ­¥æå–é¡µé¢å†…å®¹

        Returns:
            å…ƒç»„åŒ…å«:
            - é¡µé¢å†…å®¹æ®µè½åˆ—è¡¨
            - ä¸‹ä¸€ç« é“¾æ¥(ä»…åœ¨ä¸»é¡µé¢è¿”å›)
            - ç« èŠ‚æ ‡é¢˜(ä»…åœ¨ä¸»é¡µé¢è¿”å›)
        """
        html_content = await self.fetch_with_retry(session, url)
        if not html_content:
            return [], None, None

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # æå–æ ‡é¢˜ï¼ˆä»…ä¸»é¡µé¢ï¼‰
            chapter_title = None
            title_selectors = [
                'h1', 'h2', '.title', '.chapter-title',
                '.post-title', '.entry-title', 'title'
            ]

            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = self._clean_text(title_element.get_text())
                    if title and len(title) > 0:
                        chapter_title = title
                        break

            # æŸ¥æ‰¾åŒ…å«æ–‡ç« å†…å®¹çš„div
            article_div = soup.find('div', class_='blurstxt')
            if not article_div:
                return [], None, chapter_title

            # æå–æ‰€æœ‰æ®µè½
            paragraphs = article_div.find_all('p')
            if not paragraphs:
                return [], None, chapter_title

            # æå–å¹¶æ¸…ç†æ–‡æœ¬å†…å®¹
            content_list = []
            for p_tag in paragraphs:
                text = self._clean_text(p_tag.get_text())
                if text and len(text) > 5:
                    content_list.append(text)

            # æŸ¥æ‰¾ä¸‹ä¸€ç« é“¾æ¥
            next_chapter_url = None
            next_link = soup.find('a', rel='next')
            if next_link:
                next_chapter_url = next_link.get('href')
                if next_chapter_url and not next_chapter_url.startswith('http'):
                    next_chapter_url = urljoin(self.base_url, next_chapter_url)

            return content_list, next_chapter_url, chapter_title

        except Exception as e:
            self.logger.error(f"å†…å®¹æå–å¤±è´¥: {e}, URL: {url}")
            return [], None, None

    def _extract_page_number(self, url: str) -> int:
        """ä»URLä¸­æå–é¡µç  - æ–°å¢æ–¹æ³•"""
        try:
            # æ–¹æ³•1: ä»URLè·¯å¾„ä¸­æå–é¡µç 
            # ä¾‹å¦‚: /page/2/, /2.html, /p2.html
            path_patterns = [
                r'/page/(\d+)/?',
                r'/(\d+)\.html?',
                r'/p(\d+)\.html?',
                r'page=(\d+)',
                r'p=(\d+)',
            ]

            for pattern in path_patterns:
                match = re.search(pattern, url)
                if match:
                    return int(match.group(1))

            # æ–¹æ³•2: ä»æŸ¥è¯¢å‚æ•°ä¸­æå–
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)

            for param in ['page', 'p', 'paged']:
                if param in query_params:
                    try:
                        return int(query_params[param][0])
                    except (ValueError, IndexError):
                        continue

            # æ–¹æ³•3: ä»fragmentä¸­æå–
            if parsed_url.fragment:
                match = re.search(r'(\d+)', parsed_url.fragment)
                if match:
                    return int(match.group(1))

            # å¦‚æœéƒ½æ— æ³•æå–ï¼Œè¿”å›é»˜è®¤å€¼
            return 0

        except Exception as e:
            self.logger.warning(f"é¡µç æå–å¤±è´¥: {url}, é”™è¯¯: {e}")
            return 0

    def _sort_pagination_urls(self, urls: List[str]) -> List[str]:
        """å¯¹åˆ†é¡µURLè¿›è¡Œæ™ºèƒ½æ’åº - æ–°å¢æ–¹æ³•"""
        try:
            # ä¸ºæ¯ä¸ªURLåˆ†é…é¡µç 
            url_with_pages = []
            for url in urls:
                page_num = self._extract_page_number(url)
                url_with_pages.append((page_num, url))

            # æŒ‰é¡µç æ’åº
            url_with_pages.sort(key=lambda x: x[0])

            # è¿”å›æ’åºåçš„URLåˆ—è¡¨
            sorted_urls = [url for page_num, url in url_with_pages]

            self.logger.info(f"åˆ†é¡µURLæ’åºå®Œæˆ: {len(sorted_urls)} ä¸ªé¡µé¢")
            for i, (page_num, url) in enumerate(url_with_pages):
                self.logger.debug(f"  é¡µé¢ {i + 1}: é¡µç  {page_num}, URL: {url}")

            return sorted_urls

        except Exception as e:
            self.logger.error(f"åˆ†é¡µURLæ’åºå¤±è´¥: {e}")
            return urls  # å¦‚æœæ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ—è¡¨

    def _deduplicate_preserve_order(self, urls: List[str]) -> List[str]:
        """å»é‡ä½†ä¿æŒé¡ºåº - æ–°å¢æ–¹æ³•"""
        seen = set()
        result = []
        for url in urls:
            if url not in seen:
                seen.add(url)
                result.append(url)
        return result

    async def get_pagination_links(self, session: ClientSession, url: str) -> List[str]:
        """å¼‚æ­¥è·å–åˆ†é¡µé“¾æ¥ - ä¿®å¤ç‰ˆæœ¬"""
        html_content = await self.fetch_with_retry(session, url)
        if not html_content:
            return []

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            pagination_urls = []

            # æ‰©å±•åˆ†é¡µé“¾æ¥æŸ¥æ‰¾ç­–ç•¥
            pagination_selectors = [
                # åŸæœ‰çš„é€‰æ‹©å™¨
                'a.post-page-numbers',
                # é¢å¤–çš„å¸¸è§åˆ†é¡µé€‰æ‹©å™¨
                'a[class*="page"]',
                'a[href*="page"]',
                'a[href*="/p"]',
                '.pagination a',
                '.page-numbers a',
                '.wp-pagenavi a',
                '.pagenavi a',
            ]

            # å°è¯•å¤šç§é€‰æ‹©å™¨
            for selector in pagination_selectors:
                page_links = soup.select(selector)
                for link in page_links:
                    href = link.get('href')
                    if href:
                        # è¿‡æ»¤æ‰"ä¸Šä¸€é¡µ"ã€"ä¸‹ä¸€é¡µ"ç­‰å¯¼èˆªé“¾æ¥
                        link_text = link.get_text().strip().lower()
                        if any(nav_text in link_text for nav_text in ['prev', 'next', 'ä¸Šä¸€', 'ä¸‹ä¸€', 'é¦–é¡µ', 'æœ«é¡µ']):
                            continue

                        if not href.startswith('http'):
                            href = urljoin(self.base_url, href)
                        if self._validate_url(href) and href != url:  # æ’é™¤å½“å‰é¡µé¢
                            pagination_urls.append(href)

            # å»é‡ä½†ä¿æŒåŸå§‹é¡ºåº
            pagination_urls = self._deduplicate_preserve_order(pagination_urls)

            # æ™ºèƒ½æ’åº
            if pagination_urls:
                pagination_urls = self._sort_pagination_urls(pagination_urls)
                self.logger.info(f"æ‰¾åˆ°å¹¶æ’åºäº† {len(pagination_urls)} ä¸ªåˆ†é¡µé“¾æ¥")

            return pagination_urls

        except Exception as e:
            self.logger.error(f"åˆ†é¡µé“¾æ¥æå–å¤±è´¥: {e}")
            return []

    def _validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    async def crawl_complete_chapter_async(self, session: ClientSession, chapter_url: str, chapter_num: int) -> Tuple[
        Optional[str], List[str], Optional[str]]:
        """
        å¼‚æ­¥çˆ¬å–å®Œæ•´ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ‰€æœ‰åˆ†é¡µï¼‰- ä¿®å¤ç‰ˆæœ¬

        Returns:
            å…ƒç»„åŒ…å«:
            - ç« èŠ‚æ ‡é¢˜
            - å®Œæ•´ç« èŠ‚å†…å®¹
            - ä¸‹ä¸€ç« é“¾æ¥
        """
        print(f"ğŸ“– å¼€å§‹çˆ¬å–ç¬¬{chapter_num}ç« : {chapter_url}")

        # å¹¶å‘çˆ¬å–ä¸»é¡µé¢å’Œåˆ†é¡µé“¾æ¥
        main_task = self.extract_page_content(session, chapter_url)
        pagination_task = self.get_pagination_links(session, chapter_url)

        # ç­‰å¾…ä¸»é¡µé¢å’Œåˆ†é¡µé“¾æ¥è·å–å®Œæˆ
        main_result, pagination_result = await asyncio.gather(
            main_task, pagination_task, return_exceptions=True
        )

        # å¤„ç†ä¸»é¡µé¢ç»“æœ
        if isinstance(main_result, Exception):
            self.logger.error(f"ä¸»é¡µé¢çˆ¬å–å¤±è´¥: {main_result}")
            return None, [], None

        main_content, next_chapter_url, chapter_title = main_result

        # å¤„ç†åˆ†é¡µç»“æœ
        if isinstance(pagination_result, Exception):
            self.logger.warning(f"åˆ†é¡µé“¾æ¥è·å–å¤±è´¥: {pagination_result}")
            pagination_links = []
        else:
            pagination_links = pagination_result

        all_content = main_content.copy()

        if pagination_links:
            print(f"   ğŸ“„ å‘ç° {len(pagination_links)} ä¸ªåˆ†é¡µï¼ŒæŒ‰é¡ºåºçˆ¬å–ä¸­...")

            # åˆ›å»ºåˆ†é¡µçˆ¬å–ä»»åŠ¡ - æ”¹ä¸ºæŒ‰é¡ºåºå¤„ç†
            page_contents = []
            successful_pages = 0

            # æ–¹æ³•1: é¡ºåºçˆ¬å–ï¼ˆä¿è¯é¡ºåºï¼‰
            for i, page_url in enumerate(pagination_links):
                try:
                    page_content, _, _ = await self.extract_page_content(session, page_url)
                    if page_content:
                        page_contents.append((i, page_content))  # ä¿å­˜ç´¢å¼•å’Œå†…å®¹
                        successful_pages += 1
                        print(f"     âœ… åˆ†é¡µ {i + 1}/{len(pagination_links)} å®Œæˆ")
                    else:
                        print(f"     âŒ åˆ†é¡µ {i + 1}/{len(pagination_links)} å†…å®¹ä¸ºç©º")

                    # åˆ†é¡µé—´æ·»åŠ å°å»¶è¿Ÿ
                    if i < len(pagination_links) - 1:
                        await asyncio.sleep(0.5)

                except Exception as e:
                    self.logger.warning(f"åˆ†é¡µ {i + 1} çˆ¬å–å¤±è´¥: {e}")
                    continue

            # æŒ‰ç´¢å¼•é¡ºåºæ·»åŠ åˆ†é¡µå†…å®¹
            page_contents.sort(key=lambda x: x[0])  # æŒ‰ç´¢å¼•æ’åº
            for _, page_content in page_contents:
                all_content.extend(page_content)

            print(f"   âœ… åˆ†é¡µå®Œæˆï¼ŒæˆåŠŸçˆ¬å– {successful_pages}/{len(pagination_links)} ä¸ªåˆ†é¡µ")
        else:
            print("   ğŸ“„ æœ¬ç« èŠ‚æ— åˆ†é¡µ")

        total_words = sum(len(p) for p in all_content)
        print(f"   ğŸ“Š ç¬¬{chapter_num}ç« å®Œæˆï¼Œæ€»æ®µè½: {len(all_content)}ï¼Œæ€»å­—æ•°: {total_words:,}")

        # è‡ªé€‚åº”å»¶è¿Ÿ
        delay = self._calculate_adaptive_delay()
        await asyncio.sleep(delay)

        return chapter_title, all_content, next_chapter_url

    def format_chapter_content(self, title: str, all_content: List[str], chapter_num: int) -> str:
        """æ ¼å¼åŒ–ç« èŠ‚å†…å®¹"""
        formatted_content = []

        separator = "=" * 80
        formatted_content.append(separator)

        if title:
            display_title = title if len(title) <= 60 else title[:57] + "..."
            formatted_content.append(f"{display_title:^80}")
        else:
            formatted_content.append(f"ç¬¬{chapter_num}ç« ".center(80))

        formatted_content.append(separator)
        formatted_content.append("")

        for i, paragraph in enumerate(all_content):
            formatted_paragraph = f"    {paragraph}"
            formatted_content.append(formatted_paragraph)

            if i < len(all_content) - 1:
                formatted_content.append("")

        formatted_content.append("")
        formatted_content.append("")

        return "\n".join(formatted_content)

    async def save_chapter_to_file_async(self, chapter_content: str, output_file: str) -> bool:
        """å¼‚æ­¥ä¿å­˜ç« èŠ‚åˆ°æ–‡ä»¶"""
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨å¼‚æ­¥æ–‡ä»¶å†™å…¥
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._write_file_sync, chapter_content, output_file)

            return True

        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            return False

    def _write_file_sync(self, content: str, filename: str):
        """åŒæ­¥æ–‡ä»¶å†™å…¥ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        with open(filename, 'a', encoding='utf-8') as f:
            f.write(content)

    async def crawl_novel_from_chapter_async(self, start_url: str, output_file: str = None) -> bool:
        """å¼‚æ­¥çˆ¬å–å°è¯´ - ä¿®å¤ç‰ˆæœ¬"""
        # é¦–å…ˆæ£€æŸ¥åŸŸåå¯ç”¨æ€§
        if not self.check_domain_availability():
            self.logger.error("åŸŸåä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­çˆ¬å–")
            return False

        # è¿›è¡Œç½‘ç»œè¯Šæ–­
        await self.diagnose_network_issue()

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"å°è¯´_{timestamp}.txt"

        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"å°è¯´çˆ¬å–å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"èµ·å§‹ç« èŠ‚: {start_url}\n")
                f.write(f"å¹¶å‘é™åˆ¶: {self.concurrent_limit} | åŸºç¡€å»¶è¿Ÿ: {self.base_delay}s\n")
                f.write("=" * 100 + "\n\n")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return False

        print(f"ğŸš€ é«˜é€Ÿçˆ¬å–æ¨¡å¼å¯åŠ¨ï¼ˆåˆ†é¡µé¡ºåºä¿®å¤ç‰ˆï¼‰")
        print(f"ğŸ“š èµ·å§‹ç« èŠ‚: {start_url}")
        print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")
        print(f"âš¡ å¹¶å‘æ•°: {self.concurrent_limit} | åŸºç¡€å»¶è¿Ÿ: {self.base_delay}s")
        print("=" * 80)

        async with await self.create_session() as session:
            current_url = start_url
            chapter_num = 1
            self.chapter_count = 0
            self.total_words = 0
            start_time = time.time()
            consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°

            while current_url and consecutive_failures < 5:  # å¢åŠ å¤±è´¥é€€å‡ºæœºåˆ¶
                try:
                    # å¼‚æ­¥çˆ¬å–ç« èŠ‚
                    chapter_title, all_content, next_chapter_url = await self.crawl_complete_chapter_async(
                        session, current_url, chapter_num
                    )

                    if not all_content:
                        self.logger.warning(f"ç« èŠ‚ {chapter_num} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        consecutive_failures += 1
                        current_url = next_chapter_url
                        chapter_num += 1

                        # å¦‚æœè¿ç»­å¤±è´¥ï¼Œå¢åŠ å»¶è¿Ÿ
                        if consecutive_failures > 2:
                            await asyncio.sleep(10)
                        continue

                    consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°

                    # æ ¼å¼åŒ–å¹¶ä¿å­˜ç« èŠ‚
                    chapter_content = self.format_chapter_content(chapter_title, all_content, chapter_num)

                    if await self.save_chapter_to_file_async(chapter_content, output_file):
                        self.chapter_count += 1
                        chapter_words = sum(len(p) for p in all_content)
                        self.total_words += chapter_words

                        elapsed_time = time.time() - start_time
                        avg_time_per_chapter = elapsed_time / self.chapter_count if self.chapter_count > 0 else 0

                        print(f"âœ… ç¬¬{chapter_num}ç«  å·²ä¿å­˜")
                        print(f"   ğŸ“– æ ‡é¢˜: {chapter_title if chapter_title else 'æœªçŸ¥'}")
                        print(f"   ğŸ“Š å­—æ•°: {chapter_words:,} | æ€»å­—æ•°: {self.total_words:,}")
                        print(
                            f"   â±ï¸  å¹³å‡è€—æ—¶: {avg_time_per_chapter:.1f}s/ç«  | æœåŠ¡å™¨è´Ÿè½½ç³»æ•°: {self.server_load_factor:.1f}")
                        print()

                    current_url = next_chapter_url
                    chapter_num += 1

                    if not next_chapter_url:
                        print("ğŸ‰ å·²åˆ°è¾¾æœ€åä¸€ç« ï¼Œçˆ¬å–å®Œæˆï¼")
                        break

                except KeyboardInterrupt:
                    print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†çˆ¬å–è¿‡ç¨‹")
                    break
                except Exception as e:
                    self.logger.error(f"çˆ¬å–ç« èŠ‚ {chapter_num} æ—¶å‡ºé”™: {e}")
                    consecutive_failures += 1
                    current_url = next_chapter_url
                    chapter_num += 1

                    # é”™è¯¯åå¢åŠ å»¶è¿Ÿ
                    await asyncio.sleep(5 * consecutive_failures)
                    continue

        # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        try:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write("\n" + "=" * 100 + "\n")
                f.write(f"çˆ¬å–å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»ç« èŠ‚æ•°: {self.chapter_count}\n")
                f.write(f"æ€»å­—æ•°: {self.total_words:,}\n")
                f.write(f"æ€»è€—æ—¶: {total_time:.1f}ç§’\n")
                f.write(f"å¹³å‡é€Ÿåº¦: {self.chapter_count / (total_time / 60):.1f}ç« /åˆ†é’Ÿ\n")
                f.write(f"å¤±è´¥URLæ•°: {len(self.failed_urls)}\n")
                f.write("=" * 100 + "\n")
        except Exception as e:
            self.logger.error(f"å†™å…¥ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

        print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"   ğŸ“š æ€»ç« èŠ‚æ•°: {self.chapter_count}")
        print(f"   ğŸ“– æ€»å­—æ•°: {self.total_words:,}")
        print(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"   ğŸš€ å¹³å‡é€Ÿåº¦: {self.chapter_count / (total_time / 60):.1f}ç« /åˆ†é’Ÿ")
        print(f"   âŒ å¤±è´¥è¯·æ±‚: {len(self.failed_urls)}")
        print(f"   ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")

        return True


# ç½‘ç»œè¯Šæ–­å·¥å…·ç±»
class NetworkDiagnostic:
    """ç‹¬ç«‹çš„ç½‘ç»œè¯Šæ–­å·¥å…·"""

    def __init__(self, target_url: str):
        self.target_url = target_url
        self.parsed_url = urlparse(target_url)
        self.host = self.parsed_url.hostname
        self.port = self.parsed_url.port or (443 if self.parsed_url.scheme == 'https' else 80)

    def quick_diagnose(self):
        """å¿«é€Ÿè¯Šæ–­"""
        print(f"ğŸ”§ å¿«é€Ÿç½‘ç»œè¯Šæ–­: {self.target_url}")
        print("=" * 50)

        # DNSè§£ææµ‹è¯•
        try:
            ip = socket.gethostbyname(self.host)
            print(f"âœ… DNSè§£ææˆåŠŸ: {self.host} -> {ip}")
        except socket.gaierror as e:
            print(f"âŒ DNSè§£æå¤±è´¥: {e}")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. æ›´æ¢DNSæœåŠ¡å™¨(8.8.8.8)")
            print("   3. æ¸…é™¤DNSç¼“å­˜: ipconfig /flushdns")
            return False

        # ç«¯å£è¿é€šæ€§æµ‹è¯•
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)
            result = sock.connect_ex((self.host, self.port))
            sock.close()

            if result == 0:
                print(f"âœ… ç«¯å£ {self.port} è¿é€š")
                return True
            else:
                print(f"âŒ ç«¯å£ {self.port} ä¸é€š")
                print("ğŸ’¡ å¯èƒ½åŸå› :")
                print("   1. é˜²ç«å¢™é˜»æ­¢")
                print("   2. æœåŠ¡å™¨å®•æœº")
                print("   3. ç½‘ç»œé™åˆ¶")
                return False
        except Exception as e:
            print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜é€Ÿå°è¯´çˆ¬è™«å·¥å…· - åˆ†é¡µé¡ºåºä¿®å¤ç‰ˆæœ¬")
    print("=" * 60)

    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        start_url = input("è¯·è¾“å…¥èµ·å§‹ç« èŠ‚URL: ").strip()
        if not start_url:
            start_url = 'https://www.twinfoo.com/post/193117.html'
            print(f"ä½¿ç”¨é»˜è®¤URL: {start_url}")

        # å…ˆè¿›è¡Œç½‘ç»œè¯Šæ–­
        print("\nğŸ” è¿›è¡Œç½‘ç»œè¿æ¥æµ‹è¯•...")
        diagnostic = NetworkDiagnostic(start_url)
        if not diagnostic.quick_diagnose():
            print("\nâŒ ç½‘ç»œè¿æ¥æœ‰é—®é¢˜ï¼Œå»ºè®®å…ˆè§£å†³ç½‘ç»œé—®é¢˜å†è¿è¡Œçˆ¬è™«")
            return

        # è¾“å‡ºæ–‡ä»¶å
        custom_name = input("\nè¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶åï¼ˆå›è½¦ä½¿ç”¨é»˜è®¤åç§°ï¼‰: ").strip()
        if custom_name:
            if not custom_name.endswith('.txt'):
                custom_name += '.txt'
            output_file = custom_name
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"å°è¯´_{timestamp}.txt"

        # å¹¶å‘è®¾ç½®ï¼ˆé»˜è®¤é™ä½ï¼‰
        concurrent_input = input("è¯·è¾“å…¥å¹¶å‘æ•°ï¼ˆ1-5ï¼Œå›è½¦ä½¿ç”¨æ¨èå€¼2ï¼‰: ").strip()
        concurrent_limit = int(concurrent_input) if concurrent_input.isdigit() and 1 <= int(
            concurrent_input) <= 5 else 2

        # å»¶è¿Ÿè®¾ç½®ï¼ˆé»˜è®¤å¢åŠ ï¼‰
        delay_input = input("è¯·è¾“å…¥åŸºç¡€å»¶è¿Ÿæ—¶é—´/ç§’ï¼ˆ0.5-5.0ï¼Œå›è½¦ä½¿ç”¨æ¨èå€¼1.5ï¼‰: ").strip()
        try:
            base_delay = float(delay_input) if delay_input else 1.5
            base_delay = max(0.5, min(5.0, base_delay))  # é™åˆ¶èŒƒå›´
        except ValueError:
            base_delay = 1.5

        print(f"\nâš™ï¸ é…ç½®ç¡®è®¤:")
        print(f"   ğŸ”— å¹¶å‘æ•°: {concurrent_limit}")
        print(f"   â±ï¸  åŸºç¡€å»¶è¿Ÿ: {base_delay}s")
        print(f"   ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print()

        # åˆ›å»ºé«˜é€Ÿçˆ¬è™«å®ä¾‹ï¼ˆä¿®å¤ç‰ˆï¼‰
        crawler = FastNovelCrawler(
            concurrent_limit=concurrent_limit,
            base_delay=base_delay
        )

        # å¼€å§‹å¼‚æ­¥çˆ¬å–
        success = await crawler.crawl_novel_from_chapter_async(start_url, output_file)

        if success:
            print(f"\nâœ… å°è¯´çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“– æ–‡ä»¶ä¿å­˜åœ¨: {output_file}")
        else:
            print("\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


def main():
    """åŒæ­¥ä¸»å‡½æ•°å…¥å£"""
    asyncio.run(main_async())


# ç‹¬ç«‹çš„ç½‘ç»œæµ‹è¯•å·¥å…·
def test_network():
    """ç‹¬ç«‹çš„ç½‘ç»œæµ‹è¯•åŠŸèƒ½"""
    print("ğŸ”§ ç½‘ç»œè¿æ¥æµ‹è¯•å·¥å…·")
    print("=" * 40)

    url = input("è¯·è¾“å…¥è¦æµ‹è¯•çš„ç½‘ç«™URL: ").strip()
    if not url:
        url = "https://www.twinfoo.com"

    diagnostic = NetworkDiagnostic(url)
    success = diagnostic.quick_diagnose()

    if success:
        print(f"\nâœ… ç½‘ç«™ {url} è¿æ¥æ­£å¸¸")
    else:
        print(f"\nâŒ ç½‘ç«™ {url} è¿æ¥æœ‰é—®é¢˜")
        print("\nğŸ› ï¸ å¸¸è§è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("2. æ›´æ¢DNSæœåŠ¡å™¨:")
        print("   - Google DNS: 8.8.8.8, 8.8.4.4")
        print("   - 114 DNS: 114.114.114.114")
        print("3. æ¸…é™¤DNSç¼“å­˜:")
        print("   - Windows: ipconfig /flushdns")
        print("   - macOS: sudo dscacheutil -flushcache")
        print("4. ä¸´æ—¶å…³é—­é˜²ç«å¢™æµ‹è¯•")
        print("5. æ£€æŸ¥ä»£ç†è®¾ç½®")
        print("6. å°è¯•ä½¿ç”¨VPN")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        # è¿è¡Œç½‘ç»œæµ‹è¯•
        test_network()
    else:
        # è¿è¡Œä¸»ç¨‹åº
        main()

"""
åˆ†é¡µé¡ºåºä¿®å¤ç‰ˆæœ¬è¯´æ˜:

ä¸»è¦ä¿®å¤å†…å®¹:
1. âœ… æ–°å¢ _extract_page_number() æ–¹æ³•: ä»URLä¸­æ™ºèƒ½æå–é¡µç 
2. âœ… æ–°å¢ _sort_pagination_urls() æ–¹æ³•: å¯¹åˆ†é¡µURLæŒ‰é¡µç æ’åº
3. âœ… æ–°å¢ _deduplicate_preserve_order() æ–¹æ³•: å»é‡ä½†ä¿æŒé¡ºåº
4. âœ… æ”¹è¿› get_pagination_links() æ–¹æ³•: 
   - æ‰©å±•åˆ†é¡µé“¾æ¥æŸ¥æ‰¾ç­–ç•¥
   - è¿‡æ»¤å¯¼èˆªé“¾æ¥ï¼ˆä¸Šä¸€é¡µã€ä¸‹ä¸€é¡µï¼‰
   - æ™ºèƒ½æ’åºåˆ†é¡µé“¾æ¥
5. âœ… ä¿®å¤ crawl_complete_chapter_async() æ–¹æ³•:
   - æ”¹ä¸ºæŒ‰é¡ºåºçˆ¬å–åˆ†é¡µï¼ˆä¸å†å¹¶å‘ï¼‰
   - ä¿å­˜ç´¢å¼•ç¡®ä¿å†…å®¹é¡ºåºæ­£ç¡®
   - æ·»åŠ åˆ†é¡µçˆ¬å–è¿›åº¦æ˜¾ç¤º

ä¿®å¤çš„æ ¸å¿ƒé—®é¢˜:
- åŸä»£ç ä½¿ç”¨ list(set()) å»é‡ï¼Œç ´åäº†åˆ†é¡µé¡ºåº
- å¹¶å‘çˆ¬å–åˆ†é¡µæ—¶æ²¡æœ‰ä¿è¯å†…å®¹æ’å…¥é¡ºåº
- ç¼ºå°‘åˆ†é¡µURLçš„æ’åºé€»è¾‘
- åˆ†é¡µé“¾æ¥æå–ç­–ç•¥å•ä¸€

ç°åœ¨åˆ†é¡µå†…å®¹ä¼šæŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼
"""